# Backend Routes & Execution Flow

Complete map of what happens when you run a benchmark in the UI.

---

## ğŸ—ºï¸ **API Routes**

### **1. GET /**
```
Route: http://localhost:8000/
Handler: root()
Returns: index.html (the UI page)
```

### **2. GET /static/{path}**
```
Route: http://localhost:8000/static/css/style.css
Handler: StaticFiles middleware
Returns: CSS, JS, images
```

### **3. POST /api/benchmark/mojo**
```
Route: http://localhost:8000/api/benchmark/mojo
Handler: benchmark_mojo(config)
Input: { duration, n_fft, hop_length, n_mels, iterations }
Returns: { implementation, avg_time_ms, throughput_realtime, ... }
```

### **4. POST /api/benchmark/librosa**
```
Route: http://localhost:8000/api/benchmark/librosa
Handler: benchmark_librosa(config)
Input: { duration, n_fft, hop_length, n_mels, iterations }
Returns: { implementation, avg_time_ms, throughput_realtime, ... }
```

### **5. POST /api/benchmark/both** (Primary Route!)
```
Route: http://localhost:8000/api/benchmark/both
Handler: benchmark_both(config)
Input: { duration, n_fft, hop_length, n_mels, iterations }
Returns: {
  mojo: {...},
  librosa: {...},
  speedup_factor: 1.23,
  faster_percentage: 23.0,
  mojo_is_faster: true
}
```

### **6. GET /api/health**
```
Route: http://localhost:8000/api/health
Handler: health()
Returns: { status: "healthy" }
```

---

## ğŸ”„ **Complete Execution Flow**

### **User Clicks "Run Benchmark"**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. FRONTEND (main.js)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ runBenchmark() called                                           â”‚
â”‚ â”œâ”€ Collect config:                                              â”‚
â”‚ â”‚  â”œâ”€ duration: selectedDuration (from toggle buttons)          â”‚
â”‚ â”‚  â”œâ”€ n_fft: selectedFFT (from toggle buttons)                 â”‚
â”‚ â”‚  â””â”€ iterations: from number input                            â”‚
â”‚ â”‚                                                                â”‚
â”‚ â”œâ”€ Show loading overlay                                         â”‚
â”‚ â”œâ”€ Disable run button                                           â”‚
â”‚ â”‚                                                                â”‚
â”‚ â””â”€ POST /api/benchmark/both                                     â”‚
â”‚    Body: {                                                       â”‚
â”‚      duration: 30,                                              â”‚
â”‚      n_fft: 400,                                                â”‚
â”‚      hop_length: 160,  (hardcoded in JS)                        â”‚
â”‚      n_mels: 80,       (hardcoded in JS)                        â”‚
â”‚      iterations: 5                                              â”‚
â”‚    }                                                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BACKEND - benchmark_both() (app.py:166-189)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ async def benchmark_both(config):                               â”‚
â”‚   â”œâ”€ await benchmark_mojo(config)                               â”‚
â”‚   â”‚   â””â”€ Returns: mojo_result                                   â”‚
â”‚   â”‚                                                              â”‚
â”‚   â”œâ”€ await benchmark_librosa(config)                            â”‚
â”‚   â”‚   â””â”€ Returns: librosa_result                                â”‚
â”‚   â”‚                                                              â”‚
â”‚   â”œâ”€ Calculate speedup:                                         â”‚
â”‚   â”‚   speedup = librosa_ms / mojo_ms                            â”‚
â”‚   â”‚   faster_pct = ((librosa - mojo) / librosa) * 100          â”‚
â”‚   â”‚                                                              â”‚
â”‚   â””â”€ Return comparison                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MOJO    â”‚                  â”‚ LIBROSA  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## ğŸ”¥ **MOJO BENCHMARK FLOW (Detailed!)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. benchmark_mojo() (app.py:62-106)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Receives: config { duration, n_fft, hop_length, n_mels, iters }â”‚
â”‚                                                                 â”‚
â”‚ Builds command:                                                 â”‚
â”‚ cmd = [                                                         â”‚
â”‚   "python",                                                     â”‚
â”‚   "ui/backend/run_benchmark.py",                                â”‚
â”‚   "mojo",                                                       â”‚
â”‚   str(config.duration),      # e.g., "30"                       â”‚
â”‚   str(config.iterations),    # e.g., "5"                        â”‚
â”‚   str(config.n_fft),         # e.g., "400"                      â”‚
â”‚   str(config.hop_length),    # e.g., "160"                      â”‚
â”‚   str(config.n_mels)         # e.g., "80"                       â”‚
â”‚ ]                                                                â”‚
â”‚                                                                 â”‚
â”‚ subprocess.run(cmd, cwd=REPO_ROOT, timeout=120s)                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. run_benchmark.py - Python Wrapper                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Parse args:                                                     â”‚
â”‚   sys.argv[1] = "mojo"                                          â”‚
â”‚   sys.argv[2] = "30"      (duration)                            â”‚
â”‚   sys.argv[3] = "5"       (iterations)                          â”‚
â”‚   sys.argv[4] = "400"     (n_fft)                               â”‚
â”‚   sys.argv[5] = "160"     (hop_length)                          â”‚
â”‚   sys.argv[6] = "80"      (n_mels)                              â”‚
â”‚                                                                 â”‚
â”‚ Call: benchmark_mojo_single(30, 5, 400, 160, 80)                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. benchmark_mojo_single() (run_benchmark.py:11-66)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ STEP 1: Generate Mojo code dynamically                          â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                        â”‚
â”‚                                                                 â”‚
â”‚ mojo_code = f"""                                                â”‚
â”‚ from audio import mel_spectrogram                               â”‚
â”‚ from time import perf_counter_ns                                â”‚
â”‚                                                                 â”‚
â”‚ fn main() raises:                                               â”‚
â”‚     var audio = List[Float32]()                                 â”‚
â”‚     for _ in range({30 * 16000}):  # 480,000 samples           â”‚
â”‚         audio.append(0.1)                                       â”‚
â”‚                                                                 â”‚
â”‚     # Warmup                                                    â”‚
â”‚     _ = mel_spectrogram(audio,                                  â”‚
â”‚         n_fft={400}, hop_length={160}, n_mels={80})            â”‚
â”‚                                                                 â”‚
â”‚     # Benchmark                                                 â”‚
â”‚     var start = perf_counter_ns()                               â”‚
â”‚     for _ in range({5}):                                        â”‚
â”‚         _ = mel_spectrogram(audio,                              â”‚
â”‚             n_fft={400}, hop_length={160}, n_mels={80})        â”‚
â”‚     var end = perf_counter_ns()                                 â”‚
â”‚                                                                 â”‚
â”‚     var avg_ms = (end - start) / {5} / 1_000_000.0             â”‚
â”‚     print(avg_ms)                                               â”‚
â”‚ """                                                             â”‚
â”‚                                                                 â”‚
â”‚ STEP 2: Write to temp file                                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                        â”‚
â”‚ with open('/tmp/mojo_bench_temp.mojo', 'w') as f:              â”‚
â”‚     f.write(mojo_code)                                          â”‚
â”‚                                                                 â”‚
â”‚ STEP 3: Compile & Run with -O3 (CRITICAL!)                      â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                   â”‚
â”‚ subprocess.run([                                                â”‚
â”‚     'pixi', 'run', '-e', 'default',                             â”‚
â”‚     'mojo',                                                     â”‚
â”‚     '-O3',           # â† COMPILER OPTIMIZATION FLAG             â”‚
â”‚     '-I', 'src',                                                â”‚
â”‚     '/tmp/mojo_bench_temp.mojo'                                 â”‚
â”‚ ], cwd='/home/maskkiller/dev-coffee/repos/mojo-audio')         â”‚
â”‚                                                                 â”‚
â”‚ What happens:                                                   â”‚
â”‚ â”œâ”€ pixi activates conda environment                             â”‚
â”‚ â”œâ”€ mojo compiler invoked with -O3                               â”‚
â”‚ â”œâ”€ Compiles temp file (aggressive optimizations!)               â”‚
â”‚ â”œâ”€ Executes compiled binary                                     â”‚
â”‚ â”œâ”€ Benchmark runs inside compiled code:                         â”‚
â”‚ â”‚  â”œâ”€ Create audio (480k samples)                               â”‚
â”‚ â”‚  â”œâ”€ Warmup: mel_spectrogram() once                           â”‚
â”‚ â”‚  â”œâ”€ Start timer                                               â”‚
â”‚ â”‚  â”œâ”€ Loop: mel_spectrogram() 5 times                          â”‚
â”‚ â”‚  â”œâ”€ End timer                                                 â”‚
â”‚ â”‚  â”œâ”€ Calculate average                                         â”‚
â”‚ â”‚  â””â”€ Print result                                              â”‚
â”‚ â””â”€ Output: "12.345\n"                                           â”‚
â”‚                                                                 â”‚
â”‚ STEP 4: Parse output                                            â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
â”‚ avg_time = float(result.stdout.strip())  # "12.345"            â”‚
â”‚                                                                 â”‚
â”‚ STEP 5: Return                                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚ return 12.345  (in milliseconds)                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    Backend returns to
                    benchmark_both()

---

## ğŸ **LIBROSA BENCHMARK FLOW (Detailed!)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. benchmark_librosa() (app.py:109-163)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Same subprocess pattern:                                         â”‚
â”‚ cmd = [                                                         â”‚
â”‚   "python",                                                     â”‚
â”‚   "ui/backend/run_benchmark.py",                                â”‚
â”‚   "librosa",                                                    â”‚
â”‚   "30", "5", "400", "160", "80"                                 â”‚
â”‚ ]                                                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. benchmark_librosa_single() (run_benchmark.py:68-104)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ STEP 1: Create audio                                            â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
â”‚ audio = np.random.rand(30 * 16000).astype(np.float32) * 0.1    â”‚
â”‚ # 480,000 samples                                               â”‚
â”‚                                                                 â”‚
â”‚ STEP 2: Warmup                                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚ _ = librosa.feature.melspectrogram(                             â”‚
â”‚     y=audio, sr=16000,                                          â”‚
â”‚     n_fft=400, hop_length=160, n_mels=80                        â”‚
â”‚ )                                                                â”‚
â”‚                                                                 â”‚
â”‚ STEP 3: Benchmark loop                                          â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
â”‚ times = []                                                       â”‚
â”‚ for _ in range(5):                                              â”‚
â”‚     start = time.perf_counter()                                 â”‚
â”‚     _ = librosa.feature.melspectrogram(...)                     â”‚
â”‚     end = time.perf_counter()                                   â”‚
â”‚     times.append((end - start) * 1000)  # ms                    â”‚
â”‚                                                                 â”‚
â”‚ STEP 4: Calculate average                                       â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚
â”‚ avg = np.mean(times)  # Average of 5 runs                       â”‚
â”‚                                                                 â”‚
â”‚ STEP 5: Return                                                   â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚ return 14.567  (in milliseconds)                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## âš¡ **CRITICAL: Where Compilation Happens**

### **MOJO Path:**
```
User clicks â†’ Backend â†’ run_benchmark.py â†’ Generate code â†’ Write /tmp/*.mojo

                            â†“

    pixi run mojo -O3 /tmp/mojo_bench_temp.mojo
           â†“
    COMPILATION PHASE (NOT TIMED):
    â”œâ”€ Parse Mojo code
    â”œâ”€ Apply -O3 optimizations:
    â”‚  â”œâ”€ Loop unrolling
    â”‚  â”œâ”€ Inline expansion
    â”‚  â”œâ”€ Vectorization
    â”‚  â”œâ”€ Dead code elimination
    â”‚  â””â”€ Constant folding
    â””â”€ Generate optimized binary
           â†“
    EXECUTION PHASE (TIMED):
    â”œâ”€ Create audio
    â”œâ”€ Warmup: mel_spectrogram() â† NOT TIMED
    â”œâ”€ Start perf_counter_ns()    â† TIMING STARTS
    â”œâ”€ Loop 5x: mel_spectrogram()
    â”œâ”€ End perf_counter_ns()      â† TIMING ENDS
    â””â”€ Print average
```

**KEY POINT:** Compilation time is **excluded** from benchmark results!

### **LIBROSA Path:**
```
User clicks â†’ Backend â†’ run_benchmark.py

                            â†“

    Pure Python execution (no compilation)
    â”œâ”€ Import librosa (already loaded)
    â”œâ”€ Create audio
    â”œâ”€ Warmup: librosa.feature.melspectrogram() â† NOT TIMED
    â”œâ”€ Loop 5x:
    â”‚  â”œâ”€ Start time.perf_counter()  â† TIMING PER ITERATION
    â”‚  â”œâ”€ librosa.feature.melspectrogram()
    â”‚  â”œâ”€ End time.perf_counter()
    â”‚  â””â”€ Record time
    â””â”€ Average all times
```

**KEY POINT:** No compilation, but **each iteration is timed separately**!

---

## ğŸ¯ **Sources of Performance Variance**

### **Why Results Vary Run-to-Run:**

#### 1. **Mojo Compilation Variance** âŒ (Excluded from timing!)
- Compilation happens before timing starts
- Not a factor in variance

#### 2. **CPU Thermal State** âœ… (MAJOR FACTOR!)
```
Cold CPU:  Higher boost clocks â†’ Faster
Hot CPU:   Thermal throttling â†’ Slower

Variance: Â±10-20% possible!
```

#### 3. **System Load** âœ… (MODERATE FACTOR!)
```
Background processes competing for CPU
Cache pressure from other apps
Variance: Â±5-15%
```

#### 4. **Parallelization Scheduling** âœ… (MOJO SPECIFIC!)
```
Mojo uses: parallelize[] across all cores (16 cores)

Scheduler variance:
- Thread creation overhead
- Load balancing decisions
- Core migration

Variance: Â±5-10%
```

#### 5. **NumPy/librosa Caching** âœ… (PYTHON SPECIFIC!)
```
NumPy may cache:
- FFT plans (FFTW wisdom)
- Memory allocations
- JIT-compiled functions

Variance: Â±5-10%
```

#### 6. **Low Iteration Count** âœ… (MAJOR FACTOR!)
```
Current: 3-5 iterations
Problem: Small sample size = high variance

Solution: 10-20 iterations for stable average
```

---

## ğŸ”¬ **Measurement Methodology Comparison**

### **MOJO:**
```mojo
# INSIDE compiled binary (pure Mojo timing)
var start = perf_counter_ns()
for _ in range(5):
    _ = mel_spectrogram(audio, ...)  # All 5 runs timed together
var end = perf_counter_ns()

avg = (end - start) / 5
```

**Characteristics:**
- All iterations in one timing block
- Compiled code (optimized)
- Cache warm after first iteration
- Lower measurement overhead

### **LIBROSA:**
```python
# Python wrapper (per-iteration timing)
times = []
for _ in range(5):
    start = time.perf_counter()  # Time each separately
    _ = librosa.feature.melspectrogram(...)
    end = time.perf_counter()
    times.append(end - start)

avg = np.mean(times)
```

**Characteristics:**
- Each iteration timed separately
- Python interpreter overhead (minimal)
- Can detect per-run variance
- Slightly higher measurement overhead

---

## âš ï¸ **Why Performance is Close/Variable**

### **Expected with -O3:**
```
30s audio, 400 FFT:
- mojo: 10-12ms (consistently)
- librosa: 15ms (consistently)
- Gap: 25-40% faster
```

### **What You're Seeing:**
```
Run 1: librosa 2ms faster (???)
Run 2: mojo 2.5ms faster
```

**This suggests one of:**

1. **-O3 not actually being used** (would explain similar performance)
2. **Extreme system variance** (unlikely to flip winner!)
3. **Small sample size** (3-5 runs = high variance)
4. **Different FFT sizes tested** (512 is slower for mojo)

---

## ğŸ”§ **Diagnostic Steps**

### **1. Verify -O3 is Working:**
```bash
cd /home/maskkiller/dev-coffee/repos/mojo-audio

# Test wrapper directly
python ui/backend/run_benchmark.py mojo 30 10 400 160 80

# Should consistently show ~10-12ms
```

### **2. Check Compilation Output:**
Look at `/tmp/mojo_bench_temp.mojo` - verify the generated code looks correct.

### **3. Increase Iterations:**
Try 10-20 iterations instead of 5 for more stable results.

### **4. Test Command-Line Baseline:**
```bash
# Our proven benchmark
pixi run bench-optimized

# Should show ~10-12ms for 30s
# If UI shows different, something is wrong!
```

---

## ğŸ’¡ **Most Likely Issue**

**My guess:** The **pixi run mojo -O3** command in the wrapper might not be working correctly!

Let me check if pixi is properly invoking mojo with -O3...

Actually, the command is:
```python
['pixi', 'run', '-e', 'default', 'mojo', '-O3', '-I', 'src', ...]
```

This should work, but **pixi run** might not pass flags correctly!

**Better approach:** Use direct mojo path from pixi environment!

---

**Want me to investigate and fix the -O3 issue?** That's likely why performance is inconsistent! ğŸ”